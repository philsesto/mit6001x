# Unit 6: Algorithmic Complexity

# Lecture 11: Computational Complexity

## 6.1 Program Efficiency

---
### Want To Understand Efficiency Of Programs
---

- computers are fast and getting faster all the time, so maybe efficient programs don't matter?
    - conversely, data sets can be extremely large, and are getting larger all the time, as well
    - thus, simple solutions simply may not scale with size in acceptable manners
- so how could we decide which option for programs is most efficient?
- there's typically a trade off between time as an efficiency measure as well as space
    - separate **time and space efficiency** of a program
        - for example, think back to memoization, the optimization technique whereby we can store in a dictionary precomputed values so that the computation doesn't need to be run repeatedly in recursive calls or whatnot, at the expense of space in memory to contain that data structure
- for the most part, we'll be focusing on time efficiency
    - can we characterize how quickly will a program can come up with an answer to a set of inputs?
    - are there different kinds of algorithms to do it?

---
### Want To Understand Efficiency Of Programs
---

- Challenges in understanding efficiency of solution to a computational problem:
    - a program can be **implemented in many different ways**
    - you can solve a problem using only a handful of different algorithms
    - would like to separate **choices of implementation** from **choices of more abstract algorithms**

---
### How To Evaluate Efficiency Of Programs
---

- measure with a **timer**
    - some challenges here
- **count** the operations
- abstract the notion of **order of growth**
    - will argue that this is the most appropriate way of assessing the impact of choices of algorithm in solving a problem as well as in measuring the inherent difficulty in solving a problem

---
### Timing A Program
---

- use time module
- recall that imprting means to bring in that class to your own file
- **start** clock
- **call** function
- **stop** clock

```python
import time

def c_to_f(c):
    return c*9/5 + 32

t0 = time.clock()
c_tof(100000)
t1 = time.clock() - t0
print("t = ", t, ":", t1, "s,")
```

---
### Timing Programs Is Inconsistent
---

- GOAL: to evaluate different algorithms
- running time **varies between algorithms**
- running time **varies between implementations**
- running time **varies between machines**
- running time **is not predictable** based on small inputs

- time varies for different inputs, but cannot really express a relationship between inputs and time

---
### Counting Operations
---

- assume these steps take **constant time**
    - mathematical operations
    - comparisons
    - assignments
    - accessing objects in memory
- then count the number of operations executed as a function of size of input
```python
def c_to_f(c):
    return c*9.0/5 + 32

def mySum(x):
    total = 0
    for i in range(x+1):
        total += i
    return total
```

---
### Still Need A Better Way
---

- timing and counting **evaluate implementations**
- timing **evaluates machines**
- we want, however, to **evaluate algorithms**
    - evaluate **scalability**
    - evaluate **in terms of input size**

---
### Need To Choose Which Input To Use To Evaluate A Function
---

- want to express **efficiency in terms of input**, so need to decide what your input is
    - could be an integer
    - could be **length of a list**
    - **you decide** when multiple parameters to a function
- what is the input that matters?
- how am I going to measure the size of that as I talk about the efficiency of the algorithm?

---
### Different Inputs Change How The Program Runs
---

- a function that searches for an element in a list
```python
def search_for_elmt(L, e):
    for i in L:
        if i == e:
            return True
    return False
```
- when `e` is the **first element** in the list
    - BEST case
- when `e` is **not in the list**
    - WORST case
- when **look through about half** of the elements in the list
    - AVERAGE case
- we want to measure this behavior in a general way, i.e. for the worst case

---
### Best, Average, And Worst Cases
---

- suppose you are given a list `L` of some lenth `len(L)`
    - **best case**: minimum running time over all possible inputs of a given size, `len(L)`
        - constant for `search_for_elmt`
        - first element in any list
    - **average case**: average running time over all possible inputs of a given size, `len(L)`
        - practical measure
    - **worst case**: maximum running time over all possible inputs of a given size
        - linear in length of list for `search_for_elmt`
        - must search entire list and not find it
        - generally will focus on this case, as we stated previously

---
### Orders Of Growth
---

Goals:  
- want to evaluate a program's efficiency when **input is very big**
- want to express the **growth of a program's run time** as input grows
- want to put an **upper bound** on growth
- do not need to be precise: "**order of**" **not** "**exact**" growth
- we will look at **largest factors** in run time (which section of the program will take the longest to run?)

What we're going to see is that we can categorize algorithms into different classes  

---

## 6.2 Big O Notation

---
### Measuring Order Of Growth: Big O Notation
---

- Big O notation measures an **upper bound** on the **asymptotic growth** , often called the order of growth
- ***O()*** is used to describe worst case
    - worst case occurs often and is the bottleneck when a program runs
    - express rate of growth of program relative to input size
    - evaluate algorithm, not machine or implementation

---
### Exact Steps Versus *O()*
---

```python
def fact_iter(n):
    """assumes n an int >= 0"""
    answer = 1
    while n > 1:
        answer *= n
        n -= 1
    return answer
```
- computes factorial
- number of steps: `2 + 5n`
- worst case asymptotic complexity:
    - ignore additive constants
    - ignore multiplicative constants
    - as an asymptotic behavior, doesn't matter

---
### Simplification
---

- drop constants and multiplicative factors
- focus on **dominant terms**
```python
n**2 + 2*n + 2                  # O(n**2)
n**2 + 100000*n + 3**1000       # O(n**2)
log(n) + n + 4                  # O(n)
0.0001*n*log(n) + 300*n         # O(n log(n))
2*n**30 + 3*n                   # O(3**n)
```

---
### Complexity Classes: Ordered Low To High
---

| Order | Label |
| --- | --- |
| `O(1)` | constant |
| `O(log(n))` | logarithmic |
| `O(n))` | linear |
| `O(n log(n))` | loglinear |
| `O(n**c)` | polynomial |
| `O(c**n)` | exponential |

---
### Analyzing Programs And Their Complexity
---

- **combine** complexity classes
    - analyze statements inside functions
    - apply some rules, focus on dominant term

- **Law of Multiplication** for O():
    - used with **nested** statements/loops
    - O(f(n)) * O(g(n)) is O(f(n) * g(n))
    - for example:
```python
for i in range(n):
    for j in range(n):
        print('a')
```
- this example is O(n) * O(n) = O(n*n) = O(n**2) because the outer loop goes n times and the inner loop goes n times for every outer loop iteration

---

## 6.3 Complexity Classes
- c is a constant:

| complexity class | running time |
| --- | --- |
| *O(1)* | constant |
| *O(log n)* | logarithmic |
| *O(n)* | linear |
| *O(n log n)* | loglinear |
| *O(n^c)* | polynomial/quadratic |
| *O(c^n)* | exponential |

---
### Constant Complexity
---

- complexity independent of inputs
- very few interesting algorithms in this class, but often pieces of other algorithms do fit here; thus we use understanding here to help us reason about the complexity of other parts of algorithms, most usually
- can have loops or recursive calls, but number of iterations or calls independent of size of input

---
### Logarithmic Complexity
---

- complexity grows as log of size of one of its inputs
- example:
    - bisection search
    - binary search of a list
```python
def intToStr(i):
    digits = '0123456789'
    if i == 0:
        return '0'
    result = ''
    while i > 0:
        result = digits[i%10] + result
        i = i//10
    return result
```
- only have to look at loop as there are no function calls
- within while loop, constant number of steps
- how many times through loop is the only question of importance
    - each time through the loop, we reduce `i` by a factor of 10
    - how many times can one divide `i` by 10?
    - *O(log(i))*
- linear in the space complexity, logarithmic in time complexity

---
### Linear Complexity
---

- searching a list in sequence to see if an element is present
- add characters of a string, assumed to be composed of decimal digits
```python
def addDigits(s):
    val = 0
    for c in s:
        val += int(c)
    return val
```
- *O(len(s))*

- complexity case can also depend on the number of recursive calls
```python
def fact_iter(n):
    prod = 1
    for i in range(1, n-1):
        prod *= i
    return prod
```
- number of times around loop is `n`
- number of operations inside loop is a constant
- overall just *O(n)*

---
### *O()* For Recursive Factorial
---

```python
def fact_recur(n):
    """assumes n >= 0"""
    if n < = 1:
        return 1
    else:
        return n*fact_recur(n-1)
```
- computes factorial recursively
- if you time it, may notice that it runs a bit slower than iterative version, due to function calls
- still *O(n)* because the number of function calls is linear in `n`

---
### Log-Linear Complexity
---

- many practical algorithms are log-linear
- very commonly used log-linear algorithm is merge sort, which we will cover soon

---

## 6.4 Analyzing Complexity

---
### Polynomial Complexity
---

- most common polynomial algorithms are quadratic
    - i.e., complexity grows with square of size of input
- commonly occurs when we have nested loops or recursive function calls

---
### Quadratic Complexity
---

```python
def isSubset(L1, L2):
    for e1 in L1:
        matched = False
        for e2 in L2:
            if e1 == e2:
                matched = True
                break
        if not matched:
            return False
    return True
```
- nested loop here is characteristic of a quadratic order of growth
- outer loop executed `len(L1)` times
- each iteration will execute inner loop ip to `len(L2)` times
- *O(`len(L1) * len(L2)`)*
- worst case when L1 and L2 same length
- *O(len(L1)^2)*

- find intersection of two lists, return a list with each element appearing only once
```python
def intersect(L1, L2):
    tmp = []
    for e1 in L1:
        for e2 in L2:
            if e1 == e2:
                tmp.append(e1)
    res = []
    for e in tmp:
        if not(e in res):
            res.append(e)
    return res
```
- first nested loop takes `len(L1) * len(L2)` steps
- second loop takes at most `len(L1)` steps
- latter term overwhelmed by former term
- `O(len(L1) * len(L2))`

---
### *O()* For Nested Loops
---

```python
def g(n):
    """"""
    x = 0
    for i in range(n):
        for j in range(n):
            x += 1
    return x
```
- computes `n**2` very inefficiently
- wehn dealing with nested loops, **look at the ranges**
- nested loops, **each iterating `n` times**
- *O(n^2)*

---

## 6.5 More Analyzing Complexity

---
### Exponential Complexity
---

- recursive functions where more than one recursive call for each size of the problem
    - example: Towers Of Hanoi
- many important problems are inherently exponential
    - unfortunate, as the cost can be very high
    - will lead us to consider approximate solutions more quickly
- common characteristic to an exponential class algorithm is a recursive function that calls itself **multiple times** every time it recurses
```python
def genSubsets(L):
    res  = []
    if len(L) == 0:
        return [[]]                     # list of empty list
    smaller = genSubsets(L[:-1])        # all subsets without last element
    extra = L[-1:]                      # create a list of just last element
    new = []
    for small in smaller:               # for all smaller solutions, add one with the last element
        new.append(small+extra)
    return smaller+new                  # combine those with last element and those without
```
- why is this exponential in size?
- assuming append is constant time
- time includes time to solve smaller problem, plus time needed to make a copy of all elements in smaller problem
- important to think about the size of the smaller problem
- know that for a set of size `k`, there are `2**k` cases
    - so to solve, need `2**n-1 + 2**n-2 + ... + 2**0` steps
- math tells us that this is *O(2^n)*
- the characteristic here is that we're **breaking down** the problem into subproblems in which we have to make a recursive call multiple times

---

## 6.5 Recursion Complexity

---
### Tricky Complexity
---

```python
def h(n):
    """assumes n an int >= 0"""
    answer = 0
    s = str(n)
    for c in s:
        answer += int(c)
    return answer
```
- add digits of a number together
- the line "`for c in s:`" denotes linear *O(`len(s)`)*, but what in terms on input `n`?
- this is the **tricky part**:
    - convert integer to string
    - iterate over **length of string**, nt magnitude of input n
    - think of like dividing `n` by 10 each iteration
- *O(log n)* -- base doesn't matter, we're reducing the size of the problem by a constant factor at each stage

---
### Complexity Of Iterative Fibonacci
---

```python
def fib_iter(n):
    if n ==0:
        return 0
    elif n == 1:
        return 1
    else:
        fib_i = 0
        fib_ii = 1                  # everything up to this point is O(1)
        for i in range(n-1):
            tmp = fib_i
            fib_i = fib_ii
            fib_ii += tmp           # linear complexity within this block - O(n) dominant term
        return fib_ii
```
- again, keyword to picking out the complexity here is `range`
    - look **carefully** at what the `range` is telling you

---
### Complexity Of Recursive Fibonacci
---

```python
def fib_recur(n):
    """assumes n an int >= 0"""
    if n == 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fib_recur(n-1) + fib_recur(n-2)      # recursive call is TWO calls
```
- as a reduction of the problem here, we have **two** recursive calls to the function
    - as we know, that should be characteristic of exponential growth
        - if we want to solve for the fib of `n`, we've got to do that by solving two additional calls to fib

- as we saw earlier, if we use memoization to remember earlier computations (via dictionaries in Python), we can reduce this back to the efficiency of the factorial version while preserving the elegance of the recursive call

- still without memoization, we see how this problem is solved linearly with iteration and exponentially with recursion
    - that's what we want to be able to recognize -- can I find a solution that's lower complexity **and** still gives me the answer I want?

---
### When The Input Is A List
---

```python
def sum_list(L):
    total = 0
    for e in L:
        total += e
    return total
```
- *O(n)* where *n* is the length of the list
- *O(`len(L)`)
- must **define what size of input means**
    - previously, it was the magnitude of a number
    - here, it is the length of the list

---
### Big O Summary
---

- compare **efficiency of algorithms**
    - notation that describes growth 
    - **lower order of growth** is better
    - independent of machine or specific implementations

- use Big O
    - describe order of growth 
    - **asymptotic notation**
    - **upper bounds**
    - **worst case** analysis

---
### Complexity Of Common Python Functions
---

| lists: `n` is `len(L)` |  |
| --- | --- |
| `index` | *O(1)* |
| `store` | *O(1)* |
| `length`| *O(1)* |
| `append` | *O(1)* |
| `==` | *O(n)* |
| `remove` | *O(n)* |
| `copy` | *O(n)* |
| `reverse` | *O(n)* |
| `iteration` | *O(n)* |
| `in list` | *O(n)* |

| dictionaries: `n` is `len(d)` | worst case: | average case: |
| --- | --- | --- |
| `index` | *O(n)* | *O(1)* |
| `store` | *O(n)* | *O(1)* |
| `length` | *O(n)* |  |
| `delete` | *O(n)* | *O(1)* |
| `iteration` | *O(n)* | *O(n)* |

- what we see here is that we get more power with dictionaries, but it comes with a cost in terms of the complexity of the algorithm

---