# Unit 6: Algorithmic Complexity

# Lecture 12: Searching And Sorting Algorithms

## 6.6 Search Algorithms

> search algorithm
> : method for finding an item or group of items with specific properties within a collection of items

- collection could be implicit
    - example: find a square root as a search problem
        - exhaustive enumeration
        - bisection search
        - Newton-Raphson
- collection could be explicit
    - example: is a student record in a stored collection of data?

- linear search
    - **brute force** search
    - list does not have to be sorted
- bisection search
    - list ***must* be sorted** to give the correct answer
    - will see two different implementations of the algorithm

---
### Linear Search On **Unsorted** List
---

```python
def linear_search(L, e):
    found = False
    for i in range(len(L)):
        if e == L[i]:
            found = True
    return found
```
- must look through all elements to decide it's not there
- can speed up a little by returning True here, but speed up doesn't impact worst case
- *O(`len(L)`)* for the loop multiplied by *O(1)* to test `if e == L[i]`
- overall complexity is ***O(n)* - where `n` is in `len(L)`**
    - assumes we can retrieve element of list in constant time

---
### Constant Time List Access
---

- if list is all ints
    - ith element at 
        - `base + 4*l`
    - allocating fixed lenghth, say 4 bytes

- if list is heterogeneous
    - indirection
    - references to other objects
- still allocate fixed length, but follow pointer at ith location to access int

- worst case, we have to go through all the elements of the list in order to deduce if it's not present

---

## 6.7 Bisection Search

---
### Linear Search On **Sorted** List
---

```python
def search(L, e):
    for i in range(len(L)):
        if L[i] == e:
            return True
        if L[i] > e:
            return False
    return False
```
- must only look until reach a number greater than `e`
- *O(`len(L)`)* for the loop multiplied by *O(1)* to test `if e == L[i]`
- overall complexity is ***O(n)* - where `n` is in `len(L)`**
- breaking out of the loop, as we've seen, doesn't prevent the analysis that in the worst case, this is still going to be linear

---
### Use Bisection Search
---

1. Pick an index, `i`, that divides the list in half
2. Ask if `L[i] == e`
3. If not, ask if `L[i]` is larger or smaller than `e`
4. Depending on answer, search left or right half of `L` for `e`

- a new version of a divide-and-conquer algorithm
    - break into smaller version of of problem (smaller list), plus some simple operations
    - answer to a smaller version is answer to original problem

---
### Bisection Search Analysis
---

- finish looking through list when `1 = n/2`, so `i = log n`
- complexity is *O(log n)* - where `n` is in `len(L)`

---
### Bisection Search Implementation 1
---

```python
def bisect_search1(L, e):
    if L ==[]:
        return False
    elif len(L) == 1:
        return L[0] == e
    else:
        half = len(L)//2
        if L[half] > e:
            return bisct_search1(L[:half], e)
        else:
            return bisect_search1(L[half:], e)
```
- very nice alternative is to say, keep the list, but keep track of where I'm looking 
```python
def bisect_search2(L, e):
    def bisect_search_helper(L, e, low, high):
        if high == low:
            return L[low] == e
        mid = (low + high)//2
        if L[mid] == e:
            return True
        elif L[mid] > e:
            if low == mid:    # nothing left to search
                return False
            else:
                return bisect_search_helper(L, e, low, mid-1)
        else:
            return bisect_search_helper(L, e, mid+1, high)
    if len(L) == 0:    # base case here
        return False
    else:
        return bisect_search_helper(L, e, 0, len(L)-1)    
```
- helper function has two args, the lower half of the list and the upper half
    - we're simply going to change to where the pointers point

---
### Complexity Of The Two Bisection Searches
---

- **implementation1** -- `bisect_search1`
    - *O(log n)* bisection search calls
    - *O(n)* for each bisection search call
        - -> *O(n log n)*
        - -> *O(n)* for a tighter bound because length of list is healved each recursive call
- **implementation2** -- `bisect_search2` and its helper, `bisect_search_helper`
    - pass list and indices as parameters
    - list never copied, just re-passed
        - -> *O(log n)*

---
### Searching A Sorted List -- `n` Is `len(L)`
---

- using **linear search**, search for element is *O(n)*
- using **binary search**, can search for an element in *O(log n)*
    - assumes the **list is sorted**
- so when does it make sense to **sort first then search**?
    - SORT + *O(log n)* < *O(n)*
        - -> SORT < *O(n)* - *O(log n)*
    - when sorting is less than *O(n)*
        - -> never true!

---
### Amortized Cost -- `n` Is `len(L)`
---

- so why bother sorting first?
- in some cases, may **sort a list once**, then do **many searches**
- **AMORTIZE cost** of the sort over many searches
- SORT + K * *O(log n)* < K * *O(n)*
    - for large K, **SORT time becomes irrelevant**
        - always going to be the case that the log cost is much better than the linear cost
    
---

## 6.8 Bogo Sort

---
### Monkey Sort
---

- aka bogosort, stupid sort, slowsort, permutation sort, shotgun sort
    - randomly assign elements into list
    - to sort a deck of card, for example:
        - throw them up in the air
        - pick them up
        - are they sorted?
        - repeat if not sorted
    - clearly not a great way to sort anything unless you have a **very** small problem to solve

---
### Complexity Of Bogo Sort
---

```python
def bogo_sort(L):
    while not is_sorted(L):
        random.shuffle(L)
```
- best case: ***O(n)* where `n` is `len(L)`** to check if sorted
- worst case: *O(?)* as it is **unbounded** if really unlucky

---

## 6.9 Bubble Sort

- **compare consecutive pairs** of elements
- **swap elements** in pair such that smaller is first
- when reach end of list, **start over** again
- stop when **no more swaps** have been made
- largest element "bubbles" one direction or other based on comparison operation

---
### Complexity Of Bubble Sort
---

```python
def bubble_sort(L):
    swap = False
    while not swap:
        swap = True
        for j in range(1, len(L)):
            if L[j-1] > L[j]:
                swap = False
                temp = L[j]
                L[j] = L[j-1]
                L[j-1] = tmp
```
- inner loop is for doing the **comparisons**
- outer while loop is for doing **multiple passes** until no more swaps
    - ***O(n^2)* where `n` is `len(L)`** to d `len(L-1)` comparisons and `len(L) - 1` passes

---

## 6.10 Selection Sort

- first step:
    - extract **minimum element**
    - **swap it** with element at **index 0**
- subsequent step:
    - in remaining sublist, extract **minimum element**, **swap it** with the element that has **index 1**
- keep the left portion of the list sorted
    - at `i`th step, **first `i` elements in list are sorted**
    - all other elements are bigger than first `i` elements

---
### Analyzing Selection Sort
---

- loop invariant
    - given prefix of list `L[0:i]` and suffix `L[i+1:len(L)]`, then prefix is sorted and no element in prefix is larger than smallest element in suffix
        1. base case: prefix empty, suffix while list -- invariant true
        2. induction step: move minimum element from suffix to end of prefix; since invariant true before move, prefix sorted after append
        3. when exit, prefix is the entire list, suffix is empty, and the list is sorted
```python
def selection_sort(L):
    suffixSt = 0
    while suffixSt != len(L):
        for i in range(suffixSt, len(L)):
            if L[i] < L[suffixSt]:
                L[suffixSt], L[i] = L[i], L[suffixSt]
        suffixSt += 1
```
- taking advantage of the idea that we **don't want to copy the list**, we simply want to find the right pointer and then insert in the right spot
- what's the complexity here?
    - outer loop executes `len(L)` times
    - inner loop executes `len(L) - i` times
    - therefore, complexity is ***O(n^2)* where `n` is `len(L)`**

---

## 6.11 Merge Sort

- use a **divide-and-conquer** approach:
    1. `if len(L) == 0 or len(L) == 1`, already sorted
    2. if list has more than one element, split into two lists, and sort each
    3. merge sorted sublists
        - look at first element of each, move smaller to end of the result
        - when one list empty, just cop rest of other list

---
### Merging Sublists Step
---

```python
def merge(left, right):
    result = []
    i,j = 0,0
    while i < len(left) and j < len(right):
        if left[i] < right[i]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1
    while (i < len(left)):
        result.append(left[i])
        i += 1
    while (j < len(right)):
        result.append(right[j])
        j += 1
    return result
```

---
### Complexity Of Merging Sublists Step
---

- go through two lists, only one pass
- compare only rhe **smallest elements in each sublist**
- *O(`len(left)` + `len(right)`)* copied elements
- *O(`len(longer list)`)* comparisons
- **linear in length of the lists**

---
### Merge Sort -- Recursive
---

```python
def merge_sort(L):
    if len(L) < 2:
        return L[:]                         # base case
    else:
        middle = len(L)//2
        left = merge_sort(L[:middle])       # divide
        right = merge_sort(L[middle:])      # divide
        return merge(left, right)           # conquer with merge step
```
- **divide list** successively into halves
- depth-first such that **conquer smallest piece down one branch** first before moving to larger pieces

---
### Complexity Of Merge Sort
---

- at **first recursion level**
    - `n/2` elements in each list
    - *O(n) + O(n) = O(n)* where `n` is `len(L)`
- at **second recursion level**
    - `n/4` elements in each list
    - two merges: *O(n)* where `n` is `len(L)`
- each recursion level is *O(n)* where `n` is `len(L)`
- **dividing list in half** with each recursive call
    - *O(log n)* where `n` is `len(L)`
- overall complexity is ***O(n log n)* where `n` is `len(L)`**

---
### Sorting Summary
---

- bogo sort
    - randomness, unbounded *O()*
- bubble sort
    - *O(n^2)*
- selection sort
    - *O(n^2)*
    - guaranteed the first `i` elements were sorted
- merge sort
    - *O(n log n)*
- *O(n log n)* is the fastest worst-case scenario for sorting

---